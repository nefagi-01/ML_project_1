{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GD\n",
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=50\n",
    "gamma=1e-1\n",
    "w,loss = least_squares_GD(y,tX,initial_w,max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDG\n",
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=20\n",
    "gamma=1e-4\n",
    "w,loss = least_squares_SGD(y,tX,initial_w,max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LS\n",
    "w,loss = least_squares(y,tX)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RR\n",
    "lambda_=0.1\n",
    "w,loss = ridge_regression(y,tX,lambda_)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR\n",
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=300\n",
    "gamma=1e-9\n",
    "w,loss = logistic_regression(y,tX,initial_w,max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RLR\n",
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=300\n",
    "gamma=1e-9\n",
    "lambda_=0.01\n",
    "w,loss = reg_logistic_regression(y,tX, lambda_, initial_w, max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform [-1,1] into [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX[:,:][tX[:,:] == -999] = 0\n",
    "#we could normalize the data ranging from [0,1] since its binary prediction\",\n",
    "y[:,0] = (y[:,0]-min(y[:,0]))/(max(y[:,0])-min(y[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove columns containing over 50% of NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"remove_features=[]\n",
    "for i in range(tX.shape[1]):\n",
    "    col=tX[:,i]\n",
    "    total=col.shape[0]\n",
    "    counter_=collections.Counter(col)\n",
    "    nulls=counter_[-999]\n",
    "    null_percentage=round(nulls/total,2)\n",
    "    print(f'NULL percentage is: {null_percentage}')\n",
    "    if null_percentage>0.5:\n",
    "        remove_features.append(i)\n",
    "tX=np.delete(tX,remove_features,1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"remove_features=[]\n",
    "for i in range(tX.shape[1]):\n",
    "    col=tX[:,i]\n",
    "    total=col.shape[0]\n",
    "    counter_=collections.Counter(col)\n",
    "    nulls=counter_[-999]\n",
    "    null_percentage=round(nulls/total,2)\n",
    "    print(f'NULL percentage is: {null_percentage}')\n",
    "    if null_percentage>0.5:\n",
    "        remove_features.append(i)\n",
    "tX=np.delete(tX,remove_features,1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split train set into three sets according to category of jet_num (lot of column depend on this column so splitting will ensure better accuracy!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_0 = tX[:,:][(tX[:,22] == 0)]\n",
    "tX_1 = tX[:,:][(tX[:,22] == 1)]\n",
    "tX_2 = tX[:,:][(tX[:,22] >= 2)]\n",
    "\n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_0[:,j]==-999):   \n",
    "        col_to_delete.append(j)   \n",
    "tX_0 = np.delete(tX_0,col_to_delete,1)\n",
    "\n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_1[:,j]==-999):  \n",
    "        col_to_delete.append(j)\n",
    "tX_1 = np.delete(tX_1,col_to_delete,1)\n",
    " \n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_2[:,j]==-999):\n",
    "        col_to_delete.append(j)\n",
    "tX_2 = np.delete(tX_2,col_to_delete,1)\n",
    "\n",
    "\n",
    "splitted_dataset = [tX_0,tX_1,tX_2]\n",
    "\n",
    "y_0 = y[:,:][(y[:,23] == 0)]\n",
    "y_1 = y[:,:][(y[:,23] == 1)]\n",
    "y_2 = y[:,:][(y[:,23] >= 2)]\n",
    "\n",
    "y_0 = y_0[:,0]\n",
    "y_1 = y_1[:,0]\n",
    "y_2 = y_2[:,0]\n",
    "splitted_y = [y_0,y_1,y_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Removing outliers (for all 4 datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to do it for tX in general \n",
    "k = 1\n",
    "for i in range(0,tX.shape[1]):\n",
    "    q1 = np.percentile(tX[:,i],25)\n",
    "    q2 = np.percentile(tX[:,i],50)\n",
    "    q3 = np.percentile(tX[:,i],75)\n",
    "    tX[:,i][(tX[:,i] < q1 - k*(q3-q1))] = q2\n",
    "    tX[:,i][(tX[:,i] > q3 + k*(q3-q1))] = q2\n",
    "    tX[:,i][tX[:,i] == -999] = q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = []\n",
    "q2 = []\n",
    "k = 1\n",
    "for b in range(0,3):\n",
    "    for i in range(0,splitted_dataset[b].shape[1]):\n",
    "        q1 = np.percentile(splitted_dataset[b][:,i],25)\n",
    "        q2 = np.percentile(splitted_dataset[b][:,i],50)\n",
    "        q3 = np.percentile(splitted_dataset[b][:,i],75)\n",
    "        #splitted_dataset[b][:,i][(splitted_dataset[b][:,i] < q1 - k*(q3-q1))] = q2\n",
    "        #splitted_dataset[b][:,i][(splitted_dataset[b][:,i] > q3 + k*(q3-q1))] = q2\n",
    "        splitted_dataset[b][:,i][splitted_dataset[b][:,i] == -999] = q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX= standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "###for all datasets\n",
    "for i in range(0,3):\n",
    "    splitted_dataset[i] = standardize(splitted_dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove less influent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.array([5,4,28,1,3])\n",
    "arr.argsort()[:3]\n",
    "arr[arr<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply least squares\n",
    "threshold=1e-3\n",
    "w,loss = least_squares(y,tX)\n",
    "print(f'w:{w}\\nloss:{loss}')\n",
    "print(f'w:{w}\\n\\tshape:{w.shape}')\n",
    "w=w[np.abs(w)>=threshold]\n",
    "print(f'w:{w}\\n\\tshape:{w.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Cross Validation - Searching best Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.42360634e-03 -2.42360634e-03 -2.42360634e-03 -2.42360634e-03\n",
      " -2.42360634e-03 -2.42360634e-03 -2.42360634e-03 -2.42360634e-03\n",
      " -2.42360634e-03 -2.42360634e-03 -2.42360634e-03 -2.42360634e-03\n",
      " -2.42360634e-03 -2.42360634e-03 -2.42360634e-03 -2.42360634e-03\n",
      " -2.42360634e-03 -2.42360634e-03 -2.42360634e-03 -2.42360634e-03\n",
      " -2.42360634e-03 -2.42360634e-03 -2.42360634e-03 -2.42360634e-03\n",
      " -2.42360634e-03 -2.42360634e-03 -2.42360634e-03 -2.42360634e-03\n",
      " -2.42360634e-03 -2.42360634e-03  6.74130498e-03 -2.56977698e-03\n",
      " -1.46111799e-03 -7.92769521e-04 -2.24882324e-04  4.35145624e-04\n",
      " -2.50230614e-04 -9.84700999e-04 -1.27513112e-03 -1.84011344e-03\n",
      " -1.01018171e-03 -9.53753271e-04 -2.26534602e-04 -6.19412722e-04\n",
      " -9.79461765e-04 -9.80347826e-04 -1.44486178e-03 -9.79365047e-04\n",
      " -9.78382406e-04 -1.33396348e-03 -9.77718647e-04 -2.22002727e-03\n",
      " -9.93010847e-04  1.69924686e-04  2.92807869e-04  2.92722637e-04\n",
      " -5.50621480e-04 -2.28117509e-04 -2.28738629e-04 -1.73418511e-03\n",
      " -1.15696350e-02 -2.03021552e-03 -1.15434796e-03  2.59908816e-06\n",
      " -1.51634586e-03  1.84035349e-03 -1.53680751e-03 -4.00097407e-04\n",
      " -6.72100732e-04 -1.88462210e-03 -4.20871518e-04 -3.75058684e-04\n",
      " -1.51779055e-03 -2.69821408e-06 -3.95903631e-04 -3.96593718e-04\n",
      " -8.66348276e-04 -3.95841616e-04 -3.95006454e-04 -6.82519176e-04\n",
      " -3.94468540e-04 -2.73464585e-03 -4.06879284e-04 -2.38675107e-03\n",
      " -2.29878751e-03 -2.29896025e-03 -1.85964191e-03 -1.51906497e-03\n",
      " -1.51961389e-03 -1.68709167e-03  2.11273224e-02 -1.43559249e-03\n",
      " -1.12338201e-03  4.64466464e-04  2.10000017e-03  1.09616210e-02\n",
      "  2.08760649e-03 -1.62572380e-04 -3.13921096e-04 -2.57865160e-03\n",
      " -1.75275401e-04 -1.47378345e-04  2.09905662e-03  1.63827982e-04\n",
      " -1.60054527e-04 -1.60457075e-04 -5.24668026e-04 -1.60026727e-04\n",
      " -1.59495639e-04 -2.71730300e-04 -1.59168682e-04 -4.23902634e-03\n",
      " -1.66723417e-04  3.63892250e-03  3.66517151e-03  3.66500386e-03\n",
      "  1.81095148e-03  2.09828709e-03  2.09792590e-03 -2.15572002e-03\n",
      " -4.39085255e-02 -9.60690277e-04 -1.24402493e-03  1.05637623e-03\n",
      " -4.30533779e-03  8.28449700e-03 -4.31201373e-03 -6.60614470e-05\n",
      "  1.68890374e-04 -4.32844897e-03 -7.29662581e-05 -5.78655205e-05\n",
      " -4.30588361e-03  1.75414384e-04 -6.47176559e-05 -6.49260777e-05\n",
      " -3.22735530e-04 -6.47079519e-05 -6.44084059e-05  2.49427099e-04\n",
      " -6.42317498e-05 -7.63449431e-03 -6.83198373e-05 -7.21533404e-03\n",
      " -7.24968585e-03 -7.24981009e-03 -4.54146187e-03 -4.30629664e-03\n",
      " -4.30650659e-03 -3.37032989e-03]\n",
      "0.5998909357899945\n"
     ]
    }
   ],
   "source": [
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 1e-9\n",
    "splitted_dataset_poly = build_poly(tX,4)\n",
    "initial_w=np.zeros(splitted_dataset_poly.shape[1])\n",
    "w,loss = logistic_regression(y[:,0], splitted_dataset_poly, initial_w, max_iters, gamma)\n",
    "print(w)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no split\n",
    "degrees=np.arange(6)\n",
    "k_fold=3\n",
    "max_iters=50\n",
    "gamma=1e-8\n",
    "D_best = []\n",
    "rmse_tr_list_per_set=[]\n",
    "rmse_te_list_per_set=[]\n",
    "for i in range(0,1):\n",
    "    rmse_tr_list=[]\n",
    "    rmse_te_list=[]\n",
    "    for D in degrees:\n",
    "        #compute loss with cross-validation\n",
    "        rmse_tr, rmse_te=apply_cross_validation_logistic(y[:,0],tX,k_fold,D,max_iters,gamma,1)\n",
    "        rmse_tr_list.append(rmse_tr)\n",
    "        rmse_te_list.append(rmse_te)\n",
    "    D_best_index = degrees[np.argmin(np.array(rmse_te_list))]\n",
    "    D_best.append(degrees[D_best_index])\n",
    "    rmse_tr_list_per_set.append(rmse_tr_list)\n",
    "    rmse_te_list_per_set.append(rmse_te_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAME PER LO SPLIT\n",
    "degrees=np.arange(6)\n",
    "k_fold=4\n",
    "max_iters=50\n",
    "gamma=1e-8\n",
    "D_best = []\n",
    "rmse_tr_list_per_set=[]\n",
    "rmse_te_list_per_set=[]\n",
    "for i in range(0,3):\n",
    "    rmse_tr_list=[]\n",
    "    rmse_te_list=[]\n",
    "    for D in degrees:\n",
    "        #compute loss with cross-validation\n",
    "        rmse_tr, rmse_te=apply_cross_validation_logistic(splitted_y[i],splitted_dataset[i],k_fold,D,max_iters,gamma,1)\n",
    "        rmse_tr_list.append(rmse_tr)\n",
    "        rmse_te_list.append(rmse_te)\n",
    "    D_best_index = degrees[np.argmin(np.array(rmse_te_list))]\n",
    "    D_best.append(degrees[D_best_index])\n",
    "    rmse_tr_list_per_set.append(rmse_tr_list)\n",
    "    rmse_te_list_per_set.append(rmse_te_list)\n",
    "print(rmse_tr_list_per_set)\n",
    "print(rmse_te_list_per_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cross Validation Ridge Regression - Best Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree:[1] lambda_:[0.046415888336127725]\n",
      "degree:[1, 5] lambda_:[0.046415888336127725, 0.046415888336127725]\n",
      "degree:[1, 5, 4] lambda_:[0.046415888336127725, 0.046415888336127725, 1e-06]\n",
      "[1, 5, 4]\n",
      "[0.046415888336127725, 0.046415888336127725, 1e-06]\n"
     ]
    }
   ],
   "source": [
    "## PER TX SPLIT\n",
    "degrees=np.arange(6)\n",
    "lambdas=np.logspace(-6,0,10)\n",
    "lambda_best = []\n",
    "D_best = []\n",
    "k_fold=4\n",
    "for f in range(0,3):\n",
    "    rmse_tr_list=np.zeros((len(degrees),len(lambdas)))\n",
    "    rmse_te_list=np.zeros((len(degrees),len(lambdas)))\n",
    "    for i,D in enumerate(degrees):\n",
    "        for j,lambda_ in enumerate(lambdas):\n",
    "            #compute loss with cross-validation\n",
    "            rmse_tr, rmse_te=apply_cross_validation(splitted_y[f],splitted_dataset[f],k_fold,D,lambda_,1)\n",
    "            rmse_tr_list[i,j]=rmse_tr\n",
    "            rmse_te_list[i,j]=rmse_te\n",
    "    D_best_index,lambda_best_index=np.unravel_index(np.argmin(rmse_te_list),rmse_te_list.shape)\n",
    "    D_best.append(degrees[D_best_index])\n",
    "    lambda_best.append(lambdas[lambda_best_index])\n",
    "    print(f'degree:{D_best} lambda_:{lambda_best}') \n",
    "print(D_best)\n",
    "print(lambda_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees=np.arange(6)\n",
    "lambdas=np.logspace(-6,0,10)\n",
    "k_fold=10\n",
    "rmse_tr_list=np.zeros((len(degrees),len(lambdas)))\n",
    "rmse_te_list=np.zeros((len(degrees),len(lambdas)))\n",
    "for i,D in enumerate(degrees):\n",
    "    for j,lambda_ in enumerate(lambdas):\n",
    "        #compute loss with cross-validation\n",
    "        rmse_tr, rmse_te=apply_cross_validation(y,tX,k_fold,D,lambda_,1)\n",
    "        rmse_tr_list[i,j]=rmse_tr\n",
    "        rmse_te_list[i,j]=rmse_te\n",
    "D_best_index,lambda_best_index=np.unravel_index(np.argmin(rmse_te_list),rmse_te_list.shape)\n",
    "D_best=degrees[D_best_index]\n",
    "lambda_best=lambdas[lambda_best_index]\n",
    "print(f'degree:{D_best} lambda_:{lambda_best}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing ridge-regression with best Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:[-1.90075618e-03 -1.89545253e-01 -2.42255473e-01 -3.28543415e-02\n",
      "  1.16404817e+01 -3.32873453e-02  1.01347463e+00 -6.66273462e+00\n",
      "  7.95619595e-01 -7.20365383e-01 -2.01785540e-02 -9.40568745e-03\n",
      " -6.61437207e-01  1.14065194e-01 -5.24698587e-04  1.15739179e-01\n",
      " -6.72010857e-02  1.42012618e-02 -2.39680970e+00 -2.39680970e+00]\n",
      "loss:0.06803461254749035\n",
      "w:[ 0.01479825 -0.20109312 -0.20002072 -0.05231445 10.39673018  0.09922081\n",
      "  2.36638389 -6.38778018  4.04364498 -2.07654344  0.09635069 -0.06494327\n",
      " -1.9801478  -0.19727605  0.01445359  0.07548664  0.06841458 -0.01473784\n",
      " -3.70307802 -1.15285651 -0.01520057 -0.04233876 -1.14892908]\n",
      "loss:0.09315988237827781\n",
      "w:[ 1.14936600e-02 -2.93330671e-01 -2.57219899e-01  1.22018802e-01\n",
      " -3.03702354e-01  3.51978679e-02  1.94159508e-01  1.95208887e+01\n",
      " -4.42790689e-02  1.52148367e+01 -1.02485135e+01  8.82474732e+00\n",
      "  2.48046542e+01 -1.48385472e+01 -2.88215115e-01 -1.48687966e-02\n",
      " -1.46512766e+01 -3.27885358e-01  1.08221084e-01  9.86504674e-02\n",
      "  2.04737278e-01 -7.72403980e-02 -1.35907959e+01 -1.28495408e-01\n",
      "  1.85560055e-01  1.78528537e-01  6.49899124e-02  1.71618266e-01\n",
      " -7.05635700e-02 -1.51980761e+01]\n",
      "loss:0.09027951002438701\n"
     ]
    }
   ],
   "source": [
    "max_iters=300\n",
    "gamma=1e-9\n",
    "ws = []\n",
    "losses = []\n",
    "for i in range(0,len(splitted_dataset)):\n",
    "    splitted_dataset_poly = build_poly(splitted_dataset[i],D_best[i])\n",
    "    initial_w=np.zeros(splitted_dataset_poly.shape[1])\n",
    "    w,loss = ridge_regression(splitted_y[i],splitted_dataset[i],lambda_best[i])\n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with logistic (per dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###normal dataset\n",
    "max_iters=100\n",
    "gamma=1e-8\n",
    "ws = []\n",
    "losses = []\n",
    "splitted_dataset_poly = build_poly(tX,4)\n",
    "initial_w=np.zeros(splitted_dataset_poly.shape[1])\n",
    "w,loss = logistic_regression(y[:,0],splitted_dataset_poly,initial_w,max_iters,gamma)\n",
    "ws.append(w)\n",
    "losses.append(loss)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters=100\n",
    "gamma=1e-8\n",
    "ws = []\n",
    "losses = []\n",
    "for i in range(0,len(splitted_dataset)):\n",
    "    splitted_dataset_poly = build_poly(splitted_dataset[i],D_best[i])\n",
    "    initial_w=np.zeros(splitted_dataset_poly.shape[1])\n",
    "    w,loss = logistic_regression(splitted_y[i],splitted_dataset_poly,initial_w,max_iters,gamma)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "#tX_test=np.delete(tX_test,remove_features,1)\n",
    "\n",
    "ids_with_tX = np.c_[ids_test,tX_test]\n",
    "tX_test_0 = tX_test[:,:][(tX_test[:,22] == 0)] ##y check\n",
    "tX_test_1 = tX_test[:,:][(tX_test[:,22] == 1)]\n",
    "tX_test_2 = tX_test[:,:][(tX_test[:,22] >= 2)]\n",
    "\n",
    "\n",
    "###drop column also in test\n",
    "col_to_delete = []\n",
    "\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_test_0[:,j]==-999):   \n",
    "        col_to_delete.append(j)   \n",
    "tX_test_0 = np.delete(tX_test_0,col_to_delete,1)\n",
    "\n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_test_1[:,j]==-999):  \n",
    "        col_to_delete.append(j)\n",
    "tX_test_1 = np.delete(tX_test_1,col_to_delete,1)\n",
    " \n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_test_2[:,j]==-999):\n",
    "        col_to_delete.append(j)\n",
    "tX_test_2 = np.delete(tX_test_2,col_to_delete,1)\n",
    "\n",
    "##first dataset\n",
    "tX_test_0 = standardize(tX_test_0)\n",
    "tX_test_0=build_poly(tX_test_0,2)\n",
    "tX_test_1 = standardize(tX_test_1)\n",
    "tX_test_1=build_poly(tX_test_1,3)\n",
    "tX_test_2 = standardize(tX_test_2)\n",
    "tX_test_2=build_poly(tX_test_2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#rimuovere t0??\n",
    "weights=ws\n",
    "OUTPUT_PATH = 'output6.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred_test_0 = predict_labels(weights[0], tX_test_0)\n",
    "ids_0 = ids_with_tX[:,0][(ids_with_tX[:,23] == 0)]\n",
    "y_pred_test_1 = predict_labels(weights[1], tX_test_1)\n",
    "ids_1 = ids_with_tX[:,0][(ids_with_tX[:,23] == 1)]\n",
    "y_pred_test_2 = predict_labels(weights[2], tX_test_2)\n",
    "ids_2 = ids_with_tX[:,0][(ids_with_tX[:,23] >= 2)]\n",
    "#print(np.argwhere(np.isnan(y_pred_test_1)))\n",
    "#print(np.any(np.isnan(y_pred_test_1)))\n",
    "print(tX_test_1)\n",
    "print(y_pred_test_1)\n",
    "#the ids need to map to the dataset\n",
    "create_csv_submission(np.concatenate([ids_0,ids_1,ids_2]), np.concatenate([y_pred_test_0,y_pred_test_1,y_pred_test_2]), OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING ACCURACY ON DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters=300\n",
    "gamma=1e-9\n",
    "ws = []\n",
    "losses = []\n",
    "#build poly\n",
    "tX_tr=build_poly(tX,4)\n",
    "initial_w=np.zeros(tX_tr.shape[1])\n",
    "w,loss = logistic_regression(y[:,0],tX_tr,initial_w,max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7g/2ds3y3yd6319x1125r61pg1c0000gn/T/ipykernel_2907/1343039929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tX_tr' is not defined"
     ]
    }
   ],
   "source": [
    "pred = predict_labels(w, tX_tr)\n",
    "pred = (pred - min(pred))/(max(pred)-min(pred))\n",
    "accuracy(pred,y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.687716\n"
     ]
    }
   ],
   "source": [
    "##test accuracy for tx alone\n",
    "pred = predict_labels(w, splitted_dataset_poly)\n",
    "pred = (pred - min(pred))/(max(pred)-min(pred))\n",
    "print(accuracy(pred,y[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (99913,40) and (20,) not aligned: 40 (dim 1) != 20 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7g/2ds3y3yd6319x1125r61pg1c0000gn/T/ipykernel_2982/3161328703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD_best\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplitted_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/ML/ml-project-1-ml_ssg/scripts/proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(weights, data)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (99913,40) and (20,) not aligned: 40 (dim 1) != 20 (dim 0)"
     ]
    }
   ],
   "source": [
    "total_acc=[]\n",
    "for i in range(0,3):\n",
    "    pred = predict_labels(ws[i], build_poly(splitted_dataset[i],D_best[i]))\n",
    "    pred = (pred - min(pred))/(max(pred)-min(pred))\n",
    "    total_acc.append(accuracy(pred,splitted_y[i]))\n",
    "    print(accuracy(pred,splitted_y[i]))\n",
    "    \n",
    "np.mean(total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "769d560f7a9260275cfba8eac8dfb7a8ebd643a4b3237d9fce15021d62ac6fd5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
