{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (250000,31) (250000,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7g/2ds3y3yd6319x1125r61pg1c0000gn/T/ipykernel_756/1391390215.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'w:{w}\\nloss:{loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/ML/ml-project-1-ml_ssg/scripts/implementations.py\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;31m# gradient w update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/school/ML/ml-project-1-ml_ssg/scripts/implementations.py\u001b[0m in \u001b[0;36mcompute_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m    \u001b[0;31m#Compute the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (250000,31) (250000,) "
     ]
    }
   ],
   "source": [
    "#GD\n",
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=50\n",
    "gamma=1e-1\n",
    "w,loss = least_squares_GD(y,tX,initial_w,max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDG\n",
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=20\n",
    "gamma=1e-4\n",
    "w,loss = least_squares_SGD(y,tX,initial_w,max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LS\n",
    "w,loss = least_squares(y,tX)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RR\n",
    "lambda_=0.1\n",
    "w,loss = ridge_regression(y,tX,lambda_)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR\n",
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=300\n",
    "gamma=1e-9\n",
    "w,loss = logistic_regression(y,tX,initial_w,max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RLR\n",
    "initial_w=np.zeros(tX.shape[1])\n",
    "max_iters=300\n",
    "gamma=1e-9\n",
    "lambda_=0.01\n",
    "w,loss = reg_logistic_regression(y,tX, lambda_, initial_w, max_iters,gamma)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform [-1,1] into [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.     138.47    51.655 ...    1.24    -2.475  113.497]\n",
      " [   0.     160.937   68.768 ... -999.    -999.      46.226]\n",
      " [   0.    -999.     162.172 ... -999.    -999.      44.251]\n",
      " ...\n",
      " [   1.     105.457   60.526 ... -999.    -999.      41.992]\n",
      " [   0.      94.951   19.362 ... -999.    -999.       0.   ]\n",
      " [   0.    -999.      72.756 ... -999.    -999.       0.   ]]\n"
     ]
    }
   ],
   "source": [
    "#tX[:,:][tX[:,:] == -999] = 0\n",
    "#we could normalize the data ranging from [0,1] since its binary prediction\",\n",
    "y[:,0] = (y[:,0]-min(y[:,0]))/(max(y[:,0])-min(y[:,0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns containing over 50% of NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"remove_features=[]\\nfor i in range(tX.shape[1]):\\n    col=tX[:,i]\\n    total=col.shape[0]\\n    counter_=collections.Counter(col)\\n    nulls=counter_[-999]\\n    null_percentage=round(nulls/total,2)\\n    print(f'NULL percentage is: {null_percentage}')\\n    if null_percentage>0.5:\\n        remove_features.append(i)\\ntX=np.delete(tX,remove_features,1)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"remove_features=[]\n",
    "for i in range(tX.shape[1]):\n",
    "    col=tX[:,i]\n",
    "    total=col.shape[0]\n",
    "    counter_=collections.Counter(col)\n",
    "    nulls=counter_[-999]\n",
    "    null_percentage=round(nulls/total,2)\n",
    "    print(f'NULL percentage is: {null_percentage}')\n",
    "    if null_percentage>0.5:\n",
    "        remove_features.append(i)\n",
    "tX=np.delete(tX,remove_features,1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"remove_features=[]\\nfor i in range(tX.shape[1]):\\n    col=tX[:,i]\\n    total=col.shape[0]\\n    counter_=collections.Counter(col)\\n    nulls=counter_[-999]\\n    null_percentage=round(nulls/total,2)\\n    print(f'NULL percentage is: {null_percentage}')\\n    if null_percentage>0.5:\\n        remove_features.append(i)\\ntX=np.delete(tX,remove_features,1)\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"remove_features=[]\n",
    "for i in range(tX.shape[1]):\n",
    "    col=tX[:,i]\n",
    "    total=col.shape[0]\n",
    "    counter_=collections.Counter(col)\n",
    "    nulls=counter_[-999]\n",
    "    null_percentage=round(nulls/total,2)\n",
    "    print(f'NULL percentage is: {null_percentage}')\n",
    "    if null_percentage>0.5:\n",
    "        remove_features.append(i)\n",
    "tX=np.delete(tX,remove_features,1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split train set into three sets according to category of jet_num (lot of column depend on this column so splitting will ensure better accuracy!!="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 143.905   81.417   80.943 ... -999.    -999.       0.   ]\n",
      " [ 175.864   16.915  134.805 ... -999.    -999.       0.   ]\n",
      " [ 105.594   50.559  100.989 ... -999.    -999.       0.   ]\n",
      " ...\n",
      " [-999.      58.179   68.083 ... -999.    -999.       0.   ]\n",
      " [  94.951   19.362   68.812 ... -999.    -999.       0.   ]\n",
      " [-999.      72.756   70.831 ... -999.    -999.       0.   ]]\n",
      "[[ 1.43905e+02  8.14170e+01  8.09430e+01 ...  3.10820e+01  6.00000e-02\n",
      "   8.60620e+01]\n",
      " [ 1.75864e+02  1.69150e+01  1.34805e+02 ...  2.72300e+00 -8.71000e-01\n",
      "   5.31310e+01]\n",
      " [ 1.05594e+02  5.05590e+01  1.00989e+02 ...  3.77910e+01  2.40000e-02\n",
      "   1.29804e+02]\n",
      " ...\n",
      " [-9.99000e+02  5.81790e+01  6.80830e+01 ...  4.67370e+01 -8.67000e-01\n",
      "   8.04080e+01]\n",
      " [ 9.49510e+01  1.93620e+01  6.88120e+01 ...  1.21500e+01  8.11000e-01\n",
      "   1.12718e+02]\n",
      " [-9.99000e+02  7.27560e+01  7.08310e+01 ...  4.07290e+01 -1.59600e+00\n",
      "   9.94050e+01]]\n",
      "[array([0., 0., 0., ..., 0., 0., 0.]), array([0., 0., 1., ..., 0., 1., 1.]), array([1., 0., 1., ..., 0., 1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "tX_0 = tX[:,:][(tX[:,22] == 0)]\n",
    "tX_1 = tX[:,:][(tX[:,22] == 1)]\n",
    "tX_2 = tX[:,:][(tX[:,22] >= 2)]\n",
    "print(tX_0)\n",
    "\n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_0[:,j]==-999) or np.all(tX_0[:,j]==0):   \n",
    "        col_to_delete.append(j)   \n",
    "tX_0 = np.delete(tX_0,col_to_delete,1)\n",
    "\n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_1[:,j]==-999) or np.all(tX_1[:,j]==0):  \n",
    "        col_to_delete.append(j)\n",
    "tX_1 = np.delete(tX_1,col_to_delete,1)\n",
    " \n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_2[:,j]==-999) or np.all(tX_2[:,j]==0):\n",
    "        col_to_delete.append(j)\n",
    "tX_2 = np.delete(tX_2,col_to_delete,1)\n",
    "\n",
    "\n",
    "splitted_dataset = [tX_0,tX_1,tX_2]\n",
    "print(splitted_dataset[0])\n",
    "\n",
    "y_0 = y[:,:][(y[:,23] == 0)]\n",
    "y_1 = y[:,:][(y[:,23] == 1)]\n",
    "y_2 = y[:,:][(y[:,23] >= 2)]\n",
    "\n",
    "y_0 = y_0[:,0]\n",
    "y_1 = y_1[:,0]\n",
    "y_2 = y_2[:,0]\n",
    "splitted_y = [y_0,y_1,y_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers (for all 4 datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = []\n",
    "q2 = []\n",
    "k = 1\n",
    "for k in range(0,3):\n",
    "    for i in range(0,splitted_dataset[k].shape[1]):\n",
    "        q1 = np.percentile(splitted_dataset[k][:,i],25)\n",
    "        q2 = np.percentile(splitted_dataset[k][:,i],50)\n",
    "        q3 = np.percentile(splitted_dataset[k][:,i],75)\n",
    "        splitted_dataset[k][:,i][(splitted_dataset[k][:,i] < q1 - k*(q3-q1))] = q2\n",
    "        splitted_dataset[k][:,i][(splitted_dataset[k][:,i] > q3 + k*(q3-q1))] = q2\n",
    "        splitted_dataset[k][:,i][splitted_dataset[k][:,i] == -999] = q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05225226  0.09409834  0.7799777  ...  0.30120996  0.16523873\n",
      "  -0.03120384]\n",
      " [ 0.05225226  0.09409834 -0.05113073 ... -0.01761467 -1.29546857\n",
      "  -0.03120384]\n",
      " [ 0.68376754 -1.19982299 -0.05113073 ...  1.68656905  0.10875596\n",
      "   0.73141654]\n",
      " ...\n",
      " [ 0.05225226 -0.34875    -1.1956331  ... -0.01761467 -1.28919271\n",
      "  -0.03120384]\n",
      " [-0.05759768  0.09409834 -1.08364086 ... -0.01761467  1.34353217\n",
      "  -0.5305817 ]\n",
      " [ 0.05225226  1.27934592 -0.77347303 ...  2.29324442 -0.00891649\n",
      "  -1.51390037]]\n",
      "0\n",
      "[[ 0.05225226  0.09409834  0.7799777  ...  0.30120996  0.16523873\n",
      "  -0.03120384]\n",
      " [ 0.05225226  0.09409834 -0.05113073 ... -0.01761467 -1.29546857\n",
      "  -0.03120384]\n",
      " [ 0.68376754 -1.19982299 -0.05113073 ...  1.68656905  0.10875596\n",
      "   0.73141654]\n",
      " ...\n",
      " [ 0.05225226 -0.34875    -1.1956331  ... -0.01761467 -1.28919271\n",
      "  -0.03120384]\n",
      " [-0.05759768  0.09409834 -1.08364086 ... -0.01761467  1.34353217\n",
      "  -0.5305817 ]\n",
      " [ 0.05225226  1.27934592 -0.77347303 ...  2.29324442 -0.00891649\n",
      "  -1.51390037]]\n",
      "1\n",
      "[[ 2.00505292  0.83120416  1.55750782 ...  0.40785252  0.64541478\n",
      "  -0.32955199]\n",
      " [-0.08232012 -0.09722498  0.01805679 ...  1.15550632 -1.10752634\n",
      "  -0.43332826]\n",
      " [ 1.76793065 -1.08513113  1.10979075 ... -0.40285641 -0.94026517\n",
      "  -1.14862263]\n",
      " ...\n",
      " [-0.08232012  1.1428098   0.32086085 ... -0.47998636 -0.38016031\n",
      "   1.39171539]\n",
      " [ 0.92282074  1.11929487  0.80898332 ... -0.69504942  1.39533906\n",
      "   0.97056764]\n",
      " [-0.17989036  0.56051975  0.11804571 ...  1.01306926 -0.08305165\n",
      "  -0.5520273 ]]\n",
      "2\n",
      "[[ 0.92790918  0.45067241  1.13198638 ...  0.61614788 -1.36131161\n",
      "  -0.70294019]\n",
      " [-0.79732011 -0.76925958 -0.57882481 ...  0.11608109  1.71034105\n",
      "   0.29066314]\n",
      " [ 1.29203219 -0.27904573  1.57231746 ...  0.07030726 -1.52202162\n",
      "   0.11982579]\n",
      " ...\n",
      " [-0.0581688   0.01616468 -0.05148909 ...  0.25930888  0.22982758\n",
      "   0.41348309]\n",
      " [ 0.63066952 -1.07762816 -0.24124503 ...  0.29031696 -1.21821366\n",
      "  -0.21099108]\n",
      " [ 3.70910937  0.30663688 -0.41627165 ... -0.02271698 -0.62490751\n",
      "   0.04916311]]\n",
      "[[ 0.05225226  0.09409834  0.7799777  ...  0.30120996  0.16523873\n",
      "  -0.03120384]\n",
      " [ 0.05225226  0.09409834 -0.05113073 ... -0.01761467 -1.29546857\n",
      "  -0.03120384]\n",
      " [ 0.68376754 -1.19982299 -0.05113073 ...  1.68656905  0.10875596\n",
      "   0.73141654]\n",
      " ...\n",
      " [ 0.05225226 -0.34875    -1.1956331  ... -0.01761467 -1.28919271\n",
      "  -0.03120384]\n",
      " [-0.05759768  0.09409834 -1.08364086 ... -0.01761467  1.34353217\n",
      "  -0.5305817 ]\n",
      " [ 0.05225226  1.27934592 -0.77347303 ...  2.29324442 -0.00891649\n",
      "  -1.51390037]]\n"
     ]
    }
   ],
   "source": [
    "#tX,_,_ = standardize(tX)\n",
    "    \n",
    "###for all datasets\n",
    "print(splitted_dataset[0])\n",
    "for i in range(0,3):\n",
    "    splitted_dataset[i],_,_ = standardize(splitted_dataset[i])\n",
    "    print(i)\n",
    "    print(splitted_dataset[i])\n",
    "print(splitted_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Remove less influent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.array([5,4,28,1,3])\n",
    "arr.argsort()[:3]\n",
    "arr[arr<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply least squares\n",
    "threshold=1e-3\n",
    "w,loss = least_squares(y,tX)\n",
    "print(f'w:{w}\\nloss:{loss}')\n",
    "print(f'w:{w}\\n\\tshape:{w.shape}')\n",
    "w=w[np.abs(w)>=threshold]\n",
    "print(f'w:{w}\\n\\tshape:{w.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Logistic Cross Validation - Searching best Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees=np.arange(6)\n",
    "k_fold=3\n",
    "max_iters=50\n",
    "gamma=1e-8\n",
    "D_best = []\n",
    "rmse_tr_list_per_set=[]\n",
    "rmse_te_list_per_set=[]\n",
    "for i in range(0,3):\n",
    "    rmse_tr_list=[]\n",
    "    rmse_te_list=[]\n",
    "    for D in degrees:\n",
    "        #compute loss with cross-validation\n",
    "        rmse_tr, rmse_te=apply_cross_validation_logistic(splitted_y[i],splitted_dataset[i],k_fold,D,max_iters,gamma,1)\n",
    "        rmse_tr_list.append(rmse_tr)\n",
    "        rmse_te_list.append(rmse_te)\n",
    "    D_best_index = degrees[np.argmin(np.array(rmse_te_list))]\n",
    "    D_best.append(degrees[D_best_index])\n",
    "    rmse_tr_list_per_set.append(rmse_tr_list)\n",
    "    rmse_te_list_per_set.append(rmse_te_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cross Validation Ridge Regression - Best Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees=np.arange(6)\n",
    "lambdas=np.logspace(-6,0,10)\n",
    "k_fold=10\n",
    "rmse_tr_list=np.zeros((len(degrees),len(lambdas)))\n",
    "rmse_te_list=np.zeros((len(degrees),len(lambdas)))\n",
    "for i,D in enumerate(degrees):\n",
    "    for j,lambda_ in enumerate(lambdas):\n",
    "        #compute loss with cross-validation\n",
    "        rmse_tr, rmse_te=apply_cross_validation(y,tX,k_fold,D,lambda_,1)\n",
    "        rmse_tr_list[i,j]=rmse_tr\n",
    "        rmse_te_list[i,j]=rmse_te\n",
    "D_best_index,lambda_best_index=np.unravel_index(np.argmin(rmse_te_list),rmse_te_list.shape)\n",
    "D_best=degrees[D_best_index]\n",
    "lambda_best=lambdas[lambda_best_index]\n",
    "print(f'degree:{D_best} lambda_:{lambda_best}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Testing ridge-regression with best Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_tX=build_poly(tX,D_best)\n",
    "w,loss=ridge_regression(y,phi_tX,lambda_best)\n",
    "print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with logistic (per dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 18)\n",
      "w:[ 9.21697163e-04 -2.93662473e-02 -9.59564193e-03 -4.21893050e-03\n",
      "  9.82720577e-05 -4.21893083e-03  1.21522488e-02 -1.06437404e-03\n",
      "  6.07820948e-04  1.95856305e-02  5.45640414e-05 -6.60697209e-05\n",
      " -7.43337908e-03  1.27611461e-04  9.04161950e-05 -3.16142819e-03\n",
      " -1.22938033e-04  1.41403590e-03]\n",
      "loss:0.4382683554564063\n",
      "w:[ 9.74574078e-04 -1.98984310e-02 -4.13672488e-03 -2.02018964e-03\n",
      "  3.88526586e-05  4.35957700e-03  5.62121192e-03 -9.91585215e-04\n",
      "  1.27086024e-03  1.25410192e-02 -2.31870830e-05 -1.29287471e-04\n",
      " -6.17770937e-03 -1.07505115e-04  2.82471551e-05  3.09549049e-03\n",
      "  8.84673099e-05 -2.15678523e-03 -2.78360760e-04 -7.42084156e-04\n",
      " -4.34266042e-05 -7.31041505e-06 -7.42081678e-04]\n",
      "loss:0.5760233098983201\n",
      "w:[ 7.24189752e-04 -2.65249656e-02 -2.23049364e-03  2.77588152e-02\n",
      " -1.90122076e-04  6.52034577e-03  5.20470013e-04  2.71336613e-04\n",
      " -1.46072556e-02 -1.11086829e-03 -9.31569986e-04  9.30300312e-04\n",
      "  3.96486864e-04  2.80770147e-02 -1.01679837e-04 -3.50848452e-05\n",
      " -4.49827869e-03 -8.30238079e-05  8.56111124e-05  9.39266480e-03\n",
      "  1.48389568e-04 -1.44605671e-02 -3.29891909e-04 -4.73170612e-03\n",
      "  4.12940839e-05  5.96975985e-05 -1.54768650e-03  6.87213585e-05\n",
      " -9.35694210e-05 -2.46895816e-02]\n",
      "loss:1.5990867565781495\n"
     ]
    }
   ],
   "source": [
    "max_iters=300\n",
    "gamma=1e-9\n",
    "ws = []\n",
    "losses = []\n",
    "for i in range(0,len(splitted_dataset)):\n",
    "    initial_w=np.zeros(splitted_dataset[i].shape[1])\n",
    "    w,loss = logistic_regression(splitted_y[i],splitted_dataset[i],initial_w,max_iters,gamma)\n",
    "    ws.append(w)\n",
    "    losses.append(loss)\n",
    "    print(f'w:{w}\\nloss:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "#tX_test=np.delete(tX_test,remove_features,1)\n",
    "\n",
    "ids_with_tX = np.c_[ids_test,tX_test]\n",
    "print(tX_test)\n",
    "tX_test_0 = tX_test[:,:][(tX_test[:,22] == 0)]\n",
    "tX_test_1 = tX_test[:,:][(tX_test[:,22] == 1)]\n",
    "tX_test_2 = tX_test[:,:][(tX_test[:,22] >= 2)]\n",
    "print(tX_test_2.shape)\n",
    "\n",
    "\n",
    "###drop column also in test\n",
    "col_to_delete = []\n",
    "print(tX.shape)\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_test_0[:,j]==-999) or np.all(tX_test_0[:,j]==0):   \n",
    "        col_to_delete.append(j)   \n",
    "tX_test_0 = np.delete(tX_test_0,col_to_delete,1)\n",
    "\n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_test_1[:,j]==-999) or np.all(tX_test_1[:,j]==0):  \n",
    "        col_to_delete.append(j)\n",
    "tX_test_1 = np.delete(tX_test_1,col_to_delete,1)\n",
    " \n",
    "col_to_delete = []\n",
    "for j in range(0,tX.shape[1]):\n",
    "    if np.all(tX_test_2[:,j]==-999) or np.all(tX_test_2[:,j]==0):\n",
    "        col_to_delete.append(j)\n",
    "tX_test_2 = np.delete(tX_test_2,col_to_delete,1)\n",
    "\n",
    "##first dataset\n",
    "tX_test_0,_,_ = standardize(tX_test_0)\n",
    "#tX_test_0=build_poly(tX_test_0,1)\n",
    "tX_test_1,_,_ = standardize(tX_test_1)\n",
    "#tX_test_1=build_poly(tX_test_1,1)\n",
    "tX_test_2,_,_ = standardize(tX_test_2)\n",
    "#tX_test_2=build_poly(tX_test_2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 9.21697163e-04, -2.93662473e-02, -9.59564193e-03, -4.21893050e-03,\n",
      "        9.82720577e-05, -4.21893083e-03,  1.21522488e-02, -1.06437404e-03,\n",
      "        6.07820948e-04,  1.95856305e-02,  5.45640414e-05, -6.60697209e-05,\n",
      "       -7.43337908e-03,  1.27611461e-04,  9.04161950e-05, -3.16142819e-03,\n",
      "       -1.22938033e-04,  1.41403590e-03]), array([ 9.74574078e-04, -1.98984310e-02, -4.13672488e-03, -2.02018964e-03,\n",
      "        3.88526586e-05,  4.35957700e-03,  5.62121192e-03, -9.91585215e-04,\n",
      "        1.27086024e-03,  1.25410192e-02, -2.31870830e-05, -1.29287471e-04,\n",
      "       -6.17770937e-03, -1.07505115e-04,  2.82471551e-05,  3.09549049e-03,\n",
      "        8.84673099e-05, -2.15678523e-03, -2.78360760e-04, -7.42084156e-04,\n",
      "       -4.34266042e-05, -7.31041505e-06, -7.42081678e-04]), array([ 7.24189752e-04, -2.65249656e-02, -2.23049364e-03,  2.77588152e-02,\n",
      "       -1.90122076e-04,  6.52034577e-03,  5.20470013e-04,  2.71336613e-04,\n",
      "       -1.46072556e-02, -1.11086829e-03, -9.31569986e-04,  9.30300312e-04,\n",
      "        3.96486864e-04,  2.80770147e-02, -1.01679837e-04, -3.50848452e-05,\n",
      "       -4.49827869e-03, -8.30238079e-05,  8.56111124e-05,  9.39266480e-03,\n",
      "        1.48389568e-04, -1.44605671e-02, -3.29891909e-04, -4.73170612e-03,\n",
      "        4.12940839e-05,  5.96975985e-05, -1.54768650e-03,  6.87213585e-05,\n",
      "       -9.35694210e-05, -2.46895816e-02])]\n",
      "[[ 0.27826716  0.59028492  0.13476103 ... -0.30336378 -0.45387404\n",
      "  -0.38947125]\n",
      " [ 0.21743388  0.07667457 -0.38115362 ... -1.80753129  0.76808649\n",
      "  -0.62390738]\n",
      " [ 0.22960939  0.81533604 -0.20588736 ...  1.42168002 -0.67755121\n",
      "  -0.4447747 ]\n",
      " ...\n",
      " [ 0.24997082 -0.75545166 -0.85783908 ... -0.79205256  1.37025188\n",
      "   3.21453305]\n",
      " [ 0.28914922 -0.61929517 -0.31352525 ... -0.19016468 -1.54856996\n",
      "   1.01174145]\n",
      " [ 0.24966075 -0.7329354  -0.36197648 ...  0.80819613  1.46666446\n",
      "  -0.75960792]]\n",
      "[nan nan nan ... nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "#rimuovere t0??\n",
    "weights=ws\n",
    "OUTPUT_PATH = 'output6.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred_test_0 = predict_labels(weights[0], tX_test_0)\n",
    "ids_0 = ids_with_tX[:,0][(ids_with_tX[:,23] == 0)]\n",
    "y_pred_test_1 = predict_labels(weights[1], tX_test_1)\n",
    "ids_1 = ids_with_tX[:,0][(ids_with_tX[:,23] == 1)]\n",
    "y_pred_test_2 = predict_labels(weights[2], tX_test_2)\n",
    "ids_2 = ids_with_tX[:,0][(ids_with_tX[:,23] >= 2)]\n",
    "#print(np.argwhere(np.isnan(y_pred_test_1)))\n",
    "#print(np.any(np.isnan(y_pred_test_1)))\n",
    "print(tX_test_1)\n",
    "print(y_pred_test_1)\n",
    "\n",
    "#the ids need to map to the dataset\n",
    "#create_csv_submission(np.concatenate([ids_0,ids_1,ids_2]), np.concatenate([y_pred_test_0,y_pred_test_1,y_pred_test_2]), OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "769d560f7a9260275cfba8eac8dfb7a8ebd643a4b3237d9fce15021d62ac6fd5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
